ğŸ“š AI Book Recommendation System

Semantic Search Â· Emotion-Aware Ranking Â· Vector Database

ğŸš€ Overview

This project is an LLM-powered semantic book recommendation system that goes beyond traditional keyword-based search by understanding meaning, intent, and emotional context.

Book descriptions and user queries are embedded into a shared vector space using Sentence Transformers, enabling accurate semantic retrieval. Results are further refined using emotion-based ranking, category filtering, and pagination to ensure relevance and scalability.

âœ¨ Key Features

ğŸ” Semantic Search (LLM Embeddings)

Uses sentence-level embeddings instead of keyword matching

Handles abstract and descriptive user queries


ğŸ˜Š Emotion-Aware Ranking

Re-ranks books using emotion scores (joy, sadness, anger, fear, surprise)

Enables mood-based recommendations


ğŸ§  Persistent Vector Database

Uses ChromaDB for vector storage

Embeddings are generated once and reused across sessions


Lazy-loaded images

Pagination for large result sets

ğŸ“– Detail View with Similar Books

Dedicated book detail page

Similar books retrieved via semantic similarity


ğŸ—ï¸ System Architecture
User Query
   â†“
Sentence Embedding Model (MiniLM)
   â†“
Vector Similarity Search (ChromaDB)
   â†“
Candidate Books
   â†“
Emotion + Category Re-Ranking
   â†“
Paginated UI Results



ğŸ§° Tech Stack
Layer	Technology
UI	Streamlit
Embeddings	Sentence Transformers (all-MiniLM-L6-v2)
Vector Database	ChromaDB
Data Processing	Pandas, NumPy
Styling	Custom CSS (Grid-based Cards)


ğŸ§¬ Embedding Strategy

Model: all-MiniLM-L6-v2

Why this model?

Lightweight and fast

Strong semantic performance

Suitable for real-time recommendation systems

Batch Processing

Embeddings generated in batches

Prevents memory spikes during initialization


ğŸ—„ï¸ Vector Database Design

ğŸ’¾ Persistent Storage

Embeddings stored on disk (chroma_db/)

No recomputation on application restart


ğŸ†” ID Strategy

ISBN-13 used as unique document ID

Prevents duplicate embeddings


ğŸ“ Similarity Metric

Cosine similarity (HNSW index)


ğŸ“„ Pagination Strategy

Results are fetched once per query

Pagination handled at the UI layer

Prevents repeated vector database queries

Ensures consistent performance with large datasets



ğŸ¨ UI Design Decisions

Fixed-height cards for alignment consistency

CSS Grid-based layout

Lazy image loading for performance

Hover animations for better UX

Fallback image for missing thumbnails




â–¶ï¸ How to Run
1ï¸âƒ£ Install Dependencies
pip install streamlit pandas numpy chromadb sentence-transformers 
Or,
python install -r requirements.txt

2ï¸âƒ£ Start the App
streamlit run app.py


âš ï¸ On first run, embeddings are generated and stored locally.
Subsequent runs reuse existing embeddings automatically.


âš™ï¸ Performance Considerations

Embeddings cached using st.cache_resource

Dataset cached using st.cache_data

Vector search uses HNSW indexing (efficient similarity search)

UI pagination prevents DOM overload



ğŸ¯ Project Motivation

This project was built to explore real-world usage of LLM embeddings and vector databases with an emphasis on:

Practical scalability

Semantic understanding

Clean system architecture

Production-style design decisions
